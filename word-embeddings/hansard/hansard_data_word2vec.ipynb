{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from numpy import linspace\n",
    "from adjustText import adjust_text\n",
    "\n",
    "os.chdir('/users/sbuongiorno/democracy-lab/util/')\n",
    "from pyfunctions.interval_subsetter import interval_subset\n",
    "from pyfunctions.parallelize_operation import parallelize_operation\n",
    "from pyfunctions.preprocess_functions import standardize_spelling_df\n",
    "from pyfunctions.str_functions import lemmatize_df_text, str_split_df_sentences\n",
    "from pyfunctions.w2v_gensim_functions import w2v_export_gensim_models, w2v_embeddings, w2v_visualize_scatter_plot\n",
    "from pyfunctions.import_kw import import_keywords_list\n",
    "\n",
    "os.chdir('/users/sbuongiorno/democracy-lab/word-embeddings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = '/users/sbuongiorno/democracy-lab/word-embeddings/hansard_subsets/'\n",
    "\n",
    "if not os.path.exists(target_folder):\n",
    "    os.mkdir(target_folder)\n",
    "    \n",
    "    hansard = pd.read_csv('~/hansard_justnine_w_year.csv')\n",
    "    hansard = hansard[['text','year']].copy()\n",
    "    \n",
    "    hansard = hansard.rename(columns = {'text': 'speech'})\n",
    "    \n",
    "    hansard = standardize_spelling_df(hansard, \n",
    "                                  text_col='speech', \n",
    "                                  fpath_replace_list='/users/sbuongiorno/preprocess_propertywords.csv')\n",
    "    \n",
    "    interval_subset(hansard, 'year', 1800, 1910, 5, fname='hansard')\n",
    "    \n",
    "    w2v_export_gensim_models('hansard_subsets/', n_cores=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = import_keywords_list('/users/sbuongiorno/propertywords_cleaned_for_w2v_1grams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keywords_list:\n",
    "    keyword_context = w2v_embeddings.keyword_context('/users/sbuongiorno/democracy-lab/word-embeddings/hansard_subsets/', keyword)\n",
    "    \n",
    "    period_names = w2v_visualize_scatter_plot.define_periods(1800, 1915, 5)\n",
    "    \n",
    "    period_words = w2v_visualize_scatter_plot.collect_text_values(keyword_context, period_names)\n",
    "    \n",
    "    flat_list = w2v_visualize_scatter_plot.make_1D_list(period_words)    \n",
    "    \n",
    "    w2v_visualize_scatter_plot.w2v_scatter_plot(period_names, keyword_context, flat_list, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook hansard_data_word2vec.ipynb to script\n",
      "[NbConvertApp] Writing 11561 bytes to hansard_data_word2vec.py\n"
     ]
    }
   ],
   "source": [
    "#os.chdir('/users/sbuongiorno/democracy-lab/word-embeddings/hansard')\n",
    "#!jupyter nbconvert --to script hansard_data_word2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}