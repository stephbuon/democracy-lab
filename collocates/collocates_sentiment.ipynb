{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import re\n",
    "import os\n",
    "import csv \n",
    "import pandas as pd\n",
    "\n",
    "from afinn import Afinn\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_contains(ls, keywords_list):\n",
    "    filtered_list = []\n",
    "    \n",
    "    for string in ls:\n",
    "        for keyword in keywords_list:\n",
    "            if keyword in string:\n",
    "                filtered_list.append(string)\n",
    "                \n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def standardize_spelling_df(data, col_name, fpath_replace_list):\n",
    "    with open(fpath_replace_list, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        replace_list = list(csv_reader)\n",
    "    \n",
    "    for replace in replace_list:\n",
    "        data[col_name] = data[col_name].str.replace('\\\\b' + '(?i)' + replace[0] + '\\\\b', replace[1])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def import_data(data, sep, col_name, **kwargs):\n",
    "    preprocess_spelling = kwargs.get('preprocess_spelling', None)\n",
    "    fpath_replace_list = kwargs.get('fpath_replace_list', None)\n",
    "    \n",
    "    hansard = pd.read_csv(data, sep=sep, usecols=[col_name])\n",
    "    \n",
    "    if preprocess_spelling is not None:\n",
    "        hansard = standardize_spelling_df(hansard, col_name, fpath_replace_list)\n",
    "    \n",
    "    debate_text = hansard[col_name].tolist()\n",
    "    return debate_text\n",
    "\n",
    "\n",
    "def grammatical_collocates(ls, keywords_list, **kwargs):\n",
    "    return_type = kwargs.get('return_type', None)\n",
    "    \n",
    "    if type(keywords_list) != list:\n",
    "        raise TypeError('keywords_list must be a list.')\n",
    "        \n",
    "    regex = re.compile('|'.join(keywords_list))\n",
    "    \n",
    "    collocates = []\n",
    "    \n",
    "    for string in ls:\n",
    "        doc = nlp(string)\n",
    "        \n",
    "        for token in doc:\n",
    "            if regex.match(token.text):\n",
    "                col = str(token.text) + ' ' + str(token.head.text)\n",
    "                collocates.append(str(token.text) + ' ' + str(token.head.text))\n",
    "                \n",
    "                for child in token.children:\n",
    "                    collocates.append(str(token.text) + ' ' + str(child))\n",
    "                    \n",
    "    if return_type == 'ls':\n",
    "        return collocates\n",
    "    if return_type == 'df':\n",
    "        return pd.DataFrame(collocates, columns =['grammatical_collocates'])\n",
    "    \n",
    "    \n",
    "def afinn_sentiment(text):\n",
    "    return Afinn().score(text)\n",
    "\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "\n",
    "def sentiment_score(df, col_name):\n",
    "    df['afinn'] = df[col_name].apply(afinn_sentiment)\n",
    "    df['textblob'] = df[col_name].apply(textblob_sentiment)\n",
    "    df['vader'] = df[col_name].apply(vader_sentiment)\n",
    "    df['vader'] = df['vader'].apply(lambda score_dict: score_dict['compound'])    \n",
    "    return df\n",
    "\n",
    "\n",
    "class commandline:\n",
    "    \n",
    "    def argument_parser():\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description = \"For sentiments of grammatical collocates.\")\n",
    "        \n",
    "        parser.add_argument('--data', help = \"Name of data.\", required = True, default = '')\n",
    "        parser.add_argument('--sep', help = \"Delimiter.\", required = True, default = '')\n",
    "        parser.add_argument('--col_name', help = 'Name of column from which collocates will be extracted.', required = True, default = '')\n",
    "        parser.add_argument('--keywords_list', help = \"List of keywords to guide collocate extraction.\", required = True, default = '')\n",
    "        parser.add_argument('--preprocess_spelling', help = \"For standardizing spelling.\", required = False, default = '')\n",
    "        parser.add_argument('--fpath_replace_list', help = \"Name of spelling standardization file.\", required = False, default = '')\n",
    "        \n",
    "        argument = parser.parse_args()\n",
    "        \n",
    "        return argument\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'commandline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-bde028cd1cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mcli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommandline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margument_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'commandline' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        cli = commandline.argument_parser()\n",
    "        \n",
    "        input_file = cli.data\n",
    "        sep = cli.sep # fix this so it autodetects\n",
    "        col_name = cli.col_name\n",
    "        keywords_list = cli.keywords_list\n",
    "        ps = cli.preprocess_spelling\n",
    "        f = cli.fpath_replace_list\n",
    "        \n",
    "    except IndexError:\n",
    "        exit('Check commandline arguments')\n",
    "\n",
    "    export_folder = 'collocates_sentiment'\n",
    "\n",
    "    if not os.path.exists(export_folder):\n",
    "        os.mkdir(export_folder)\n",
    "        \n",
    "    keywords_list = pd.read_csv(keywords_list)\n",
    "    kw_col_name = keywords_list.columns[0]\n",
    "    keywords_list = keywords_list[kw_col_name].tolist()\n",
    "    \n",
    "    if ps is not None:\n",
    "        data = import_data(input_file, sep, col_name, preprocess_spelling=ps, fpath_replace_list=f)\n",
    "    else:\n",
    "        data = import_data(input_file, sep, col_name)\n",
    "    \n",
    "    data = list_contains(data, keywords_list)\n",
    "    \n",
    "    data = grammatical_collocates(data, keywords_list, return_type='df') # add a cli option for return type \n",
    "    \n",
    "    data = sentiment_score(data, 'grammatical_collocates')\n",
    "    \n",
    "    data.to_csv(export_folder + '/' + 'collocates_sentiment_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Untitled2.ipynb to script\n",
      "[NbConvertApp] Writing 5557 bytes to Untitled2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Untitled2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = import_data('/users/sbuongiorno/hansard_justnine_w_year.csv', ',', 'text', preprocess_spelling=True, fpath_replace_list='/users/sbuongiorno/preprocess_propertywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = list_contains(test, ['respect', ' it '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keywords_list must be a list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f305e9428029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrammatical_collocates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'he'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-3e3b820f59ef>\u001b[0m in \u001b[0;36mgrammatical_collocates\u001b[0;34m(ls, keywords_list, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keywords_list must be a list.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: keywords_list must be a list."
     ]
    }
   ],
   "source": [
    "#save = grammatical_collocates(out, 'he', return_type='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = sentiment_score(save, 'grammatical_collocates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
