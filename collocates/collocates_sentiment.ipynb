{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv \n",
    "import pandas as pd\n",
    "\n",
    "from afinn import Afinn\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "\n",
    "#def list_contains(ls, keywords_list):\n",
    "#    filtered_list = []\n",
    "    \n",
    "#    for string in ls:\n",
    "#        for keyword in keywords_list:\n",
    "#            if keyword in string:\n",
    "#                filtered_list.append(string)\n",
    "                \n",
    "#    return filtered_list\n",
    "\n",
    "def data_contains(dic, keywords_list):\n",
    "    filtered_dic = {}\n",
    "    \n",
    "    regex = re.compile('|'.join(keywords_list))\n",
    "    \n",
    "    for key, value in dic.items():\n",
    "        for sentence in value:\n",
    "            if regex.search(sentence):\n",
    "                if key in filtered_dic:\n",
    "                    filtered_dic[key].append(sentence)\n",
    "                else:\n",
    "                    filtered_dic[key] = [sentence]\n",
    "    \n",
    "    return filtered_dic\n",
    "\n",
    "\n",
    "def standardize_spelling_df(data, text_col, fpath_replace_list):\n",
    "    with open(fpath_replace_list, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        replace_list = list(csv_reader)\n",
    "        \n",
    "    data[text_col] = data[text_col].str.lower()\n",
    "    \n",
    "    for replace in replace_list:\n",
    "        data[text_col] = data[text_col].str.replace('\\\\b' + '(?i)' + replace[0] + '\\\\b', replace[1])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def import_data(fpath, sep, text_col, year_col, **kwargs): #col_name, **kwargs):\n",
    "    preprocess_spelling = kwargs.get('preprocess_spelling', None)\n",
    "    fpath_replace_list = kwargs.get('fpath_replace_list', None)\n",
    "    \n",
    "    #data = pd.read_csv(fpath, sep=sep, usecols=[col_name])\n",
    "    \n",
    "    data = pd.read_csv(fpath, sep=sep)\n",
    "    \n",
    "    data = data[[text_col, year_col]]\n",
    "    \n",
    "    if preprocess_spelling is not None:\n",
    "        data = standardize_spelling_df(data, text_col, fpath_replace_list)\n",
    "\n",
    "    \n",
    "    year = []\n",
    "    text = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        year.append(row['year'])\n",
    "        text.append(row['text'])\n",
    "        \n",
    "    year_text_list = list(zip(year, text))\n",
    "    \n",
    "    year_text_dict = {}\n",
    "    \n",
    "    for (tup1, tup2) in year_text_list:\n",
    "        if tup1 in year_text_dict:\n",
    "            year_text_dict[tup1].append(tup2)\n",
    "        else:\n",
    "            year_text_dict[tup1] = [tup2]\n",
    "            \n",
    "    year_text_dict = {str(key): value for key, value in year_text_dict.items()}\n",
    "\n",
    "    #return col\n",
    "    return year_text_dict\n",
    "\n",
    "\n",
    "# def grammatical_collocates(ls, keywords_list, **kwargs):\n",
    "#     return_type = kwargs.get('return_type', None)\n",
    "    \n",
    "#     if type(keywords_list) != list:\n",
    "#         raise TypeError('keywords_list must be a list.')\n",
    "        \n",
    "#     regex = re.compile('|'.join(keywords_list))\n",
    "    \n",
    "#     collocates = []\n",
    "    \n",
    "#     for string in ls:\n",
    "#         doc = nlp(string)\n",
    "        \n",
    "#         for token in doc:\n",
    "#             if regex.match(token.text):\n",
    "#                 col = str(token.text) + ' ' + str(token.head.text)\n",
    "#                 collocates.append(str(token.text) + ' ' + str(token.head.text))\n",
    "                \n",
    "#                 for child in token.children:\n",
    "#                     collocates.append(str(token.text) + ' ' + str(child))\n",
    "                    \n",
    "#     if return_type == 'ls':\n",
    "#         return collocates\n",
    "#     if return_type == 'df':\n",
    "#         return pd.DataFrame(collocates, columns =['grammatical_collocates'])\n",
    "    \n",
    "\n",
    "\n",
    "def grammatical_collocates(dic, keywords_list, **kwargs):\n",
    "    \n",
    "    if type(keywords_list) != list:\n",
    "        raise TypeError('keywords_list must be a list.')\n",
    "        \n",
    "    regex = re.compile('|'.join(keywords_list))\n",
    "    \n",
    "    all_collocates_df = pd.DataFrame()\n",
    "    \n",
    "    for key, value in dic.items():\n",
    "        year = key\n",
    "        \n",
    "        for sentence in value:\n",
    "            doc = nlp(sentence)\n",
    "            \n",
    "            collocates = []\n",
    "            for token in doc:\n",
    "                if regex.match(token.text):\n",
    "                    if token.text != token.head.text:\n",
    "                        collocates.append(str(token.text) + ' ' + str(token.head.text))\n",
    "                \n",
    "                    for child in token.children:\n",
    "                        collocates.append(str(token.text) + ' ' + str(child))\n",
    "                        \n",
    "            collocates_df = pd.DataFrame(collocates, columns=['grammatical_collocates'])\n",
    "            collocates_df['year'] = year\n",
    "            \n",
    "            all_collocates_df = pd.concat([all_collocates_df, collocates_df], axis=0)\n",
    "\n",
    "    return all_collocates_df\n",
    "\n",
    "def afinn_sentiment(text):\n",
    "    return Afinn().score(text)\n",
    "\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "\n",
    "def sentiment_score(df, col_name):\n",
    "    df['afinn'] = df[col_name].apply(afinn_sentiment)\n",
    "    df['textblob'] = df[col_name].apply(textblob_sentiment)\n",
    "    df['vader'] = df[col_name].apply(vader_sentiment)\n",
    "    df['vader'] = df['vader'].apply(lambda score_dict: score_dict['compound'])    \n",
    "    return df\n",
    "\n",
    "\n",
    "class commandline: # need to update this to reflect current config \n",
    "    \n",
    "    def argument_parser():\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description = \"For sentiments of grammatical collocates.\")\n",
    "        \n",
    "        parser.add_argument('--data', help = \"Name of data.\", required = True, default = '')\n",
    "        parser.add_argument('--sep', help = \"Delimiter.\", required = True, default = '')\n",
    "        parser.add_argument('--col_name', help = 'Name of column from which collocates will be extracted.', required = True, default = '')\n",
    "        parser.add_argument('--keywords_list', help = \"List of keywords to guide collocate extraction.\", required = True, default = '')\n",
    "        parser.add_argument('--preprocess_spelling', help = \"For standardizing spelling.\", required = False, default = '')\n",
    "        parser.add_argument('--fpath_replace_list', help = \"Name of spelling standardization file.\", required = False, default = '')\n",
    "        \n",
    "        argument = parser.parse_args()\n",
    "        \n",
    "        return argument\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = is_notebook()\n",
    "\n",
    "if notebook == True:\n",
    "    keywords_list = pd.read_csv('propertywords_cleaned_for_collocates.csv')\n",
    "    kw_col_name = keywords_list.columns[0]\n",
    "    keywords_list = keywords_list[kw_col_name].tolist()\n",
    "\n",
    "    # can't use usecols here bc text is str andn year is int -- dumb \n",
    "    data = import_data('/users/sbuongiorno/rerun_csv_chunk.csv', ',', text_col='text', year_col='year', preprocess_spelling=True, fpath_replace_list='/users/sbuongiorno/preprocess_propertywords.csv')\n",
    "\n",
    "    data = data_contains(data, keywords_list)\n",
    "    \n",
    "    data = grammatical_collocates(data, keywords_list) # add a cli option for return type \n",
    "    \n",
    "    data = sentiment_score(data, 'grammatical_collocates')\n",
    "    \n",
    "    export_folder = 'collocates_sentiment'\n",
    "    \n",
    "    if not os.path.exists(export_folder):\n",
    "        os.mkdir(export_folder)\n",
    "    \n",
    "    data.to_csv(export_folder + '/' + 'collocates_sentiment_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to update this to match notebook section -- specifically, i added col_use = text and year \n",
    "    \n",
    "#     if __name__ == '__main__':\n",
    "#         try:\n",
    "#             cli = commandline.argument_parser()\n",
    "        \n",
    "#             input_file = cli.data\n",
    "#             sep = cli.sep # fix this so it autodetects\n",
    "#             col_name = cli.col_name\n",
    "#             keywords_list = cli.keywords_list\n",
    "#             ps = cli.preprocess_spelling\n",
    "#             f = cli.fpath_replace_list\n",
    "        \n",
    "#         except IndexError:\n",
    "#             exit('Check commandline arguments')\n",
    "\n",
    "#         export_folder = 'collocates_sentiment'\n",
    "\n",
    "#         if not os.path.exists(export_folder):\n",
    "#             os.mkdir(export_folder)\n",
    "        \n",
    "#         keywords_list = pd.read_csv(keywords_list)\n",
    "#         kw_col_name = keywords_list.columns[0]\n",
    "#         keywords_list = keywords_list[kw_col_name].tolist()\n",
    "    \n",
    "#         if ps is not None:\n",
    "#             data = import_data(input_file, sep, col_name, preprocess_spelling=ps, fpath_replace_list=f)\n",
    "#         else:\n",
    "#             data = import_data(input_file, sep, col_name)\n",
    "    \n",
    "#             data = list_contains(data, keywords_list)\n",
    "    \n",
    "#             data = grammatical_collocates(data, keywords_list, return_type='df') # add a cli option for return type \n",
    "    \n",
    "#             data = sentiment_score(data, 'grammatical_collocates')\n",
    "    \n",
    "#             data.to_csv(export_folder + '/' + 'collocates_sentiment_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = ['absentee', 'adscription', 'agist', 'allod', 'allotment', 'almoign', 'amortized', 'apanage', 'atar', 'attorn', \n",
    "                 'blettro', 'bond-land', 'bookland', 'bordage', 'bordar', 'boscage', 'burgage', 'burgery', 'cablicum', \n",
    "                 'cammandery', 'cartbote', 'charter-hold', 'charter-land', 'chattel', 'co-feof', 'co-tenancy', 'co-tenant', \n",
    "                 'coedcae', 'commonage', 'commonties', 'commonty', 'croft', 'curiality', 'demesn', 'depopulation', 'dispossess',\n",
    "                 'domanial', 'domesday', 'dreng', 'eject', 'emin', 'enclosure','escuage', 'esplees', 'estover', 'ethel', 'evict',\n",
    "                 'feu', 'ffridd', 'fiar', 'fief', 'firebote', 'fogg', 'folkland', 'frank-ferm', 'frank-marriage', 'frankalmoign',\n",
    "                 'franklin', 'frith', 'gavelkind', 'gaveller', 'grasanese', 'gwely', 'haybote', 'herbage', 'holdership', \n",
    "                 'homager', 'householdership', 'hypothec', 'inclosure', 'intercommoning', 'joint-tenancy', 'joint-tenant', \n",
    "                 'knight-service', 'laen', 'laetic', 'land', 'lease', 'lessee', 'lifehold', 'liferent', 'livier', 'lotment', \n",
    "                 'mail-payer', 'majorat', 'manorialize', 'manurance', 'mese', 'mesn', 'metayage', 'metayer', 'mivvy', 'occupance',\n",
    "                 'occupancy', 'outland', 'pannage', 'parage', 'patrony', 'pendicle', 'perpetual', 'piscary', 'ploughbote', \n",
    "                 'poffle', 'pollam', 'pre-emptive', 'property', 'rack-rent', 'radknight', 'radman', 'rent', 'rere-fief', 'roture',\n",
    "                 'roturier', 'rundale', 'runrig', 'ryoti', 'ryotwar', 'scattald', 'seisin', 'severalty', 'socage', 'sokeman', \n",
    "                 'solidate', 'sorning', 'sple', 'squat', 'steelbow', 'sub-fief', 'subaltern', 'subfeu', 'sublessee', 'subsman', \n",
    "                 'subtenancy', 'subtenant', 'subvassal', 'suit-hold', 'swinamote', 'tanistic', 'tanistry', 'tariot', 'tenancy',\n",
    "                 'tenant', 'tenement', 'tenurial', 'termon', 'termor', 'terre-tenant', 'thanage', 'thaneland', 'three-life',\n",
    "                 'thring', 'turbary', 'udal', 'under-tenancy', 'under-tenant', 'underlessee', 'undertenant', 'undervassal',\n",
    "                 'unfeued', 'unleased', 'unlet', 'urbarial', 'vassal', 'venville', 'vidame', 'villan', 'villar', 'villein', \n",
    "                 'woodmote', 'zemindar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "arg1 = sys.argv[1]\n",
    "\n",
    "data = import_data(arg1, ',', text_col='text', year_col='year', preprocess_spelling=True, fpath_replace_list='/users/sbuongiorno/preprocess_propertywords.csv')\n",
    "\n",
    "data = data_contains(data, keywords_list)\n",
    "    \n",
    "data = grammatical_collocates(data, keywords_list, return_type='df') # add a cli option for return type \n",
    "\n",
    "data = sentiment_score(data, 'grammatical_collocates')\n",
    "\n",
    "if not data.empty:\n",
    "    handle = open(arg1 + '.pickle', 'wb') \n",
    "    pickle.dump(data, handle)\n",
    "else:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook collocates_sentiment.ipynb to script\n",
      "[NbConvertApp] Writing 11853 bytes to collocates_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script collocates_sentiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
