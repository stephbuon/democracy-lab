{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import re\n",
    "import os\n",
    "import csv \n",
    "import pandas as pd\n",
    "\n",
    "from afinn import Afinn\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "\n",
    "def list_contains(ls, keywords_list):\n",
    "    filtered_list = []\n",
    "    \n",
    "    for string in ls:\n",
    "        for keyword in keywords_list:\n",
    "            if ('\\\\b' + keyword + '\\\\b') in string:\n",
    "                filtered_list.append(string)\n",
    "                \n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def standardize_spelling_df(data, col_name, fpath_replace_list):\n",
    "    with open(fpath_replace_list, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        replace_list = list(csv_reader)\n",
    "    \n",
    "    for replace in replace_list:\n",
    "        data[col_name] = data[col_name].str.replace('\\\\b' + '(?i)' + replace[0] + '\\\\b', replace[1])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def import_data(fpath, sep, col_name, **kwargs):\n",
    "    preprocess_spelling = kwargs.get('preprocess_spelling', None)\n",
    "    fpath_replace_list = kwargs.get('fpath_replace_list', None)\n",
    "    \n",
    "    data = pd.read_csv(fpath, sep=sep, usecols=[col_name])\n",
    "    \n",
    "    if preprocess_spelling is not None:\n",
    "        data = standardize_spelling_df(data, col_name, fpath_replace_list)\n",
    "    \n",
    "    col = data[col_name].tolist()\n",
    "    return col\n",
    "\n",
    "\n",
    "def grammatical_collocates(ls, keywords_list, **kwargs):\n",
    "    return_type = kwargs.get('return_type', None)\n",
    "    \n",
    "    if type(keywords_list) != list:\n",
    "        raise TypeError('keywords_list must be a list.')\n",
    "        \n",
    "    regex = re.compile('|'.join(keywords_list))\n",
    "    \n",
    "    collocates = []\n",
    "    \n",
    "    for string in ls:\n",
    "        doc = nlp(string)\n",
    "        \n",
    "        for token in doc:\n",
    "            if regex.match(token.text):\n",
    "                col = str(token.text) + ' ' + str(token.head.text)\n",
    "                collocates.append(str(token.text) + ' ' + str(token.head.text))\n",
    "                \n",
    "                for child in token.children:\n",
    "                    collocates.append(str(token.text) + ' ' + str(child))\n",
    "                    \n",
    "    if return_type == 'ls':\n",
    "        return collocates\n",
    "    if return_type == 'df':\n",
    "        return pd.DataFrame(collocates, columns =['grammatical_collocates'])\n",
    "    \n",
    "    \n",
    "def afinn_sentiment(text):\n",
    "    return Afinn().score(text)\n",
    "\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "\n",
    "def sentiment_score(df, col_name):\n",
    "    df['afinn'] = df[col_name].apply(afinn_sentiment)\n",
    "    df['textblob'] = df[col_name].apply(textblob_sentiment)\n",
    "    df['vader'] = df[col_name].apply(vader_sentiment)\n",
    "    df['vader'] = df['vader'].apply(lambda score_dict: score_dict['compound'])    \n",
    "    return df\n",
    "\n",
    "\n",
    "class commandline:\n",
    "    \n",
    "    def argument_parser():\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description = \"For sentiments of grammatical collocates.\")\n",
    "        \n",
    "        parser.add_argument('--data', help = \"Name of data.\", required = True, default = '')\n",
    "        parser.add_argument('--sep', help = \"Delimiter.\", required = True, default = '')\n",
    "        parser.add_argument('--col_name', help = 'Name of column from which collocates will be extracted.', required = True, default = '')\n",
    "        parser.add_argument('--keywords_list', help = \"List of keywords to guide collocate extraction.\", required = True, default = '')\n",
    "        parser.add_argument('--preprocess_spelling', help = \"For standardizing spelling.\", required = False, default = '')\n",
    "        parser.add_argument('--fpath_replace_list', help = \"Name of spelling standardization file.\", required = False, default = '')\n",
    "        \n",
    "        argument = parser.parse_args()\n",
    "        \n",
    "        return argument\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = is_notebook()\n",
    "\n",
    "if notebook == True:\n",
    "    keywords_list = pd.read_csv('propertywords_cleaned_for_collocates.csv')\n",
    "    kw_col_name = keywords_list.columns[0]\n",
    "    keywords_list = keywords_list[kw_col_name].tolist()\n",
    "\n",
    "    data = import_data('/users/sbuongiorno/hansard_justnine_w_year.csv', ',', 'text', preprocess_spelling=True, fpath_replace_list='/users/sbuongiorno/preprocess_propertywords.csv')\n",
    "\n",
    "    data = list_contains(data, keywords_list)\n",
    "    \n",
    "    data = grammatical_collocates(data, keywords_list, return_type='df') # add a cli option for return type \n",
    "    \n",
    "    data = sentiment_score(data, 'grammatical_collocates')\n",
    "    \n",
    "    data.to_csv(export_folder + '/' + 'collocates_sentiment_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip == True\n",
    "if skip == False:\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        try:\n",
    "            cli = commandline.argument_parser()\n",
    "        \n",
    "            input_file = cli.data\n",
    "            sep = cli.sep # fix this so it autodetects\n",
    "            col_name = cli.col_name\n",
    "            keywords_list = cli.keywords_list\n",
    "            ps = cli.preprocess_spelling\n",
    "            f = cli.fpath_replace_list\n",
    "        \n",
    "        except IndexError:\n",
    "            exit('Check commandline arguments')\n",
    "\n",
    "        export_folder = 'collocates_sentiment'\n",
    "\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.mkdir(export_folder)\n",
    "        \n",
    "        keywords_list = pd.read_csv(keywords_list)\n",
    "        kw_col_name = keywords_list.columns[0]\n",
    "        keywords_list = keywords_list[kw_col_name].tolist()\n",
    "    \n",
    "        if ps is not None:\n",
    "            data = import_data(input_file, sep, col_name, preprocess_spelling=ps, fpath_replace_list=f)\n",
    "        else:\n",
    "            data = import_data(input_file, sep, col_name)\n",
    "    \n",
    "            data = list_contains(data, keywords_list)\n",
    "    \n",
    "            data = grammatical_collocates(data, keywords_list, return_type='df') # add a cli option for return type \n",
    "    \n",
    "            data = sentiment_score(data, 'grammatical_collocates')\n",
    "    \n",
    "            data.to_csv(export_folder + '/' + 'collocates_sentiment_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = pd.read_csv('propertywords_cleaned_for_collocates.csv')\n",
    "kw_col_name = keywords_list.columns[0]\n",
    "keywords_list = keywords_list[kw_col_name].tolist()\n",
    "\n",
    "arg1 = sys.argv[1]\n",
    "\n",
    "data = import_data(arg1, ',', 'text', preprocess_spelling=True, fpath_replace_list='/users/sbuongiorno/preprocess_propertywords.csv')\n",
    "\n",
    "data = list_contains(data, keywords_list)\n",
    "    \n",
    "data = grammatical_collocates(data, keywords_list, return_type='df') # add a cli option for return type \n",
    "\n",
    "data = sentiment_score(data, 'grammatical_collocates')\n",
    "\n",
    "export_folder = 'collocates_sentiment'\n",
    "\n",
    "if not os.path.exists(export_folder):\n",
    "    os.mkdir(export_folder)\n",
    "\n",
    "data.to_csv(export_folder + '/' + 'arg1', + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Untitled2.ipynb to script\n",
      "[NbConvertApp] Writing 7205 bytes to Untitled2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Untitled2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
